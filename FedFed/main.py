import argparse
import logging
import os
import socket
import sys
import time
import json
import matplotlib.pyplot as plt
import numpy as np
from datetime import datetime

import torch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))

from utils.logger import logging_config
from configs import get_cfg

from algorithms_standalone.fedavg.FedAVGManager import FedAVGManager
from algorithms_standalone.fednova.FedNovaManager import FedNovaManager
from algorithms_standalone.fedprox.FedProxManager import FedProxManager
from algorithms_standalone.pfedme.PFedMeManager import PFedMeManager

from utils.set import *

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


def add_args(parser):
    """
    parser : argparse.ArgumentParser
    return a parser added with args required by fit
    """
    parser.add_argument("--config_file", default="config_yummly28k_test.yaml", type=str,
                        help="é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨Yummly28kæ•°æ®é›†")
    parser.add_argument("--algorithm", default="FedAvg", type=str, 
                        help="Algorithm to run: FedAvg, FedProx, PFedMe, "
                             "FedNova, FedFed")
    parser.add_argument("--quick_test", action="store_true", 
                        help="Enable quick test mode")
    parser.add_argument("--full_validation", action="store_true", 
                        help="Enable full validation")
    parser.add_argument("--output_dir", default="./results", type=str,
                        help="Directory to save results")
    parser.add_argument("opts", 
                        help="Modify config options using the command-line",
                        default=None, nargs=argparse.REMAINDER)

    args = parser.parse_args()
    return args


class TrainingProgressTracker:
    """è®­ç»ƒè¿›åº¦è·Ÿè¸ªå™¨"""
    
    def __init__(self, total_rounds, output_dir, algorithm_name, dataset_name):
        self.total_rounds = total_rounds
        self.output_dir = output_dir
        self.algorithm_name = algorithm_name
        self.dataset_name = dataset_name
        self.start_time = time.time()
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        
        # è®­ç»ƒå†å²è®°å½•
        self.train_history = {
            'rounds': [],
            'train_loss': [],
            'train_acc': [],
            'test_loss': [],
            'test_acc': [],
            'time_per_round': []
        }
        
        # æ‰“å°æ•°æ®é›†å’Œç®—æ³•ä¿¡æ¯
        print("=" * 80)
        print(f"ğŸ½ï¸  ä½¿ç”¨æ•°æ®é›†: {dataset_name}")
        print(f"ğŸ¤–  è®­ç»ƒç®—æ³•: {algorithm_name}")
        print(f"ğŸ”„  æ€»è½®æ•°: {total_rounds}")
        print(f"ğŸ“  ç»“æœä¿å­˜ç›®å½•: {output_dir}")
        print("=" * 80)
        
        logging.info(f"=== å¼€å§‹è®­ç»ƒ {algorithm_name} ç®—æ³• ===")
        logging.info(f"æ•°æ®é›†: {dataset_name}")
        logging.info(f"æ€»è½®æ•°: {total_rounds}")
        logging.info(f"ç»“æœä¿å­˜ç›®å½•: {output_dir}")

    def update_progress(self, round_idx, train_metrics, test_metrics):
        """æ›´æ–°è®­ç»ƒè¿›åº¦"""
        current_time = time.time()
        elapsed_time = current_time - self.start_time
        time_per_round = elapsed_time / (round_idx + 1)
        remaining_time = time_per_round * (self.total_rounds - round_idx - 1)
        
        # è®°å½•å†å²
        self.train_history['rounds'].append(round_idx + 1)
        self.train_history['train_loss'].append(
            train_metrics.get('loss', 0))
        self.train_history['train_acc'].append(
            train_metrics.get('accuracy', 0))
        self.train_history['test_loss'].append(test_metrics.get('loss', 0))
        self.train_history['test_acc'].append(
            test_metrics.get('accuracy', 0))
        self.train_history['time_per_round'].append(time_per_round)
        
        # æ˜¾ç¤ºè¿›åº¦
        progress = (round_idx + 1) / self.total_rounds * 100
        
        # ä½¿ç”¨ä¸­æ–‡å’Œemojiæ˜¾ç¤ºè¿›åº¦
        print(f"\nğŸ”„ === è½®æ¬¡ {round_idx + 1}/{self.total_rounds} ({progress:.1f}%) ===")
        print(f"ğŸ“Š è®­ç»ƒæŸå¤±: {train_metrics.get('loss', 0):.4f}, "
              f"è®­ç»ƒç²¾åº¦: {train_metrics.get('accuracy', 0):.4f}")
        print(f"ğŸ¯ æµ‹è¯•æŸå¤±: {test_metrics.get('loss', 0):.4f}, "
              f"æµ‹è¯•ç²¾åº¦: {test_metrics.get('accuracy', 0):.4f}")
        print(f"â±ï¸  å·²ç”¨æ—¶é—´: {elapsed_time/60:.1f}åˆ†é’Ÿ, "
              f"é¢„è®¡å‰©ä½™: {remaining_time/60:.1f}åˆ†é’Ÿ")
        print("-" * 60)
        
        logging.info(f"=== è½®æ¬¡ {round_idx + 1}/{self.total_rounds} "
                     f"({progress:.1f}%) ===")
        logging.info(f"è®­ç»ƒæŸå¤±: {train_metrics.get('loss', 0):.4f}, "
                     f"è®­ç»ƒç²¾åº¦: {train_metrics.get('accuracy', 0):.4f}")
        logging.info(f"æµ‹è¯•æŸå¤±: {test_metrics.get('loss', 0):.4f}, "
                     f"æµ‹è¯•ç²¾åº¦: {test_metrics.get('accuracy', 0):.4f}")
        logging.info(f"å·²ç”¨æ—¶é—´: {elapsed_time/60:.1f}åˆ†é’Ÿ, "
                     f"é¢„è®¡å‰©ä½™: {remaining_time/60:.1f}åˆ†é’Ÿ")
        logging.info("-" * 60)

    def save_results(self):
        """ä¿å­˜è®­ç»ƒç»“æœ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ä¿å­˜è®­ç»ƒå†å²åˆ°JSONæ–‡ä»¶
        results_file = os.path.join(
            self.output_dir, f"{self.algorithm_name}_results_{timestamp}.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(self.train_history, f, indent=2, ensure_ascii=False)
        
        # ç»˜åˆ¶è®­ç»ƒæ›²çº¿
        self._plot_training_curves(timestamp)
        
        # ä¿å­˜æœ€ç»ˆç»“æœæ‘˜è¦
        self._save_summary(timestamp)
        
        print(f"ğŸ“ è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")
        logging.info(f"è®­ç»ƒç»“æœå·²ä¿å­˜åˆ°: {self.output_dir}")

    def _plot_training_curves(self, timestamp):
        """ç»˜åˆ¶è®­ç»ƒæ›²çº¿"""
        # è®¾ç½®ä¸­æ–‡å­—ä½“
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Arial Unicode MS', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False
        
        plt.figure(figsize=(15, 10))
        
        # æŸå¤±æ›²çº¿
        plt.subplot(2, 2, 1)
        plt.plot(self.train_history['rounds'], 
                 self.train_history['train_loss'], 
                 'b-', label='Training Loss', linewidth=2)
        plt.plot(self.train_history['rounds'], 
                 self.train_history['test_loss'], 
                 'r-', label='Test Loss', linewidth=2)
        plt.xlabel('Round')
        plt.ylabel('Loss')
        plt.title(f'{self.algorithm_name} on {self.dataset_name} - Loss Curves')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # ç²¾åº¦æ›²çº¿
        plt.subplot(2, 2, 2)
        plt.plot(self.train_history['rounds'], 
                 self.train_history['train_acc'], 
                 'b-', label='Training Accuracy', linewidth=2)
        plt.plot(self.train_history['rounds'], 
                 self.train_history['test_acc'], 
                 'r-', label='Test Accuracy', linewidth=2)
        plt.xlabel('Round')
        plt.ylabel('Accuracy')
        plt.title(f'{self.algorithm_name} on {self.dataset_name} - Accuracy Curves')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        # æ¯è½®æ—¶é—´
        plt.subplot(2, 2, 3)
        plt.plot(self.train_history['rounds'], 
                 self.train_history['time_per_round'], 
                 'g-', linewidth=2)
        plt.xlabel('Round')
        plt.ylabel('Time (seconds)')
        plt.title(f'{self.algorithm_name} - Training Time per Round')
        plt.grid(True, alpha=0.3)
        
        # ç²¾åº¦å¯¹æ¯”
        plt.subplot(2, 2, 4)
        final_train_acc = (self.train_history['train_acc'][-1] 
                          if self.train_history['train_acc'] else 0)
        final_test_acc = (self.train_history['test_acc'][-1] 
                         if self.train_history['test_acc'] else 0)
        plt.bar(['Training Accuracy', 'Test Accuracy'], 
                [final_train_acc, final_test_acc], 
                color=['blue', 'red'], alpha=0.7)
        plt.ylabel('Accuracy')
        plt.title(f'{self.algorithm_name} - Final Accuracy Comparison')
        plt.ylim(0, 1)
        
        plt.tight_layout()
        plot_file = os.path.join(
            self.output_dir, f"{self.algorithm_name}_curves_{timestamp}.png")
        plt.savefig(plot_file, dpi=300, bbox_inches='tight')
        plt.close()

    def _save_summary(self, timestamp):
        """ä¿å­˜ç»“æœæ‘˜è¦"""
        total_time = time.time() - self.start_time
        
        summary = {
            'algorithm': self.algorithm_name,
            'dataset': self.dataset_name,
            'timestamp': timestamp,
            'total_rounds': self.total_rounds,
            'total_time_minutes': total_time / 60,
            'final_train_accuracy': (self.train_history['train_acc'][-1] 
                                   if self.train_history['train_acc'] else 0),
            'final_test_accuracy': (self.train_history['test_acc'][-1] 
                                  if self.train_history['test_acc'] else 0),
            'best_test_accuracy': (max(self.train_history['test_acc']) 
                                 if self.train_history['test_acc'] else 0),
            'final_train_loss': (self.train_history['train_loss'][-1] 
                               if self.train_history['train_loss'] else 0),
            'final_test_loss': (self.train_history['test_loss'][-1] 
                              if self.train_history['test_loss'] else 0),
            'avg_time_per_round': (np.mean(self.train_history['time_per_round']) 
                                 if self.train_history['time_per_round'] else 0)
        }
        
        summary_file = os.path.join(
            self.output_dir, f"{self.algorithm_name}_summary_{timestamp}.json")
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)


def add_progress_tracking_to_manager(manager, tracker):
    """ä¸ºç®¡ç†å™¨æ·»åŠ è¿›åº¦è·Ÿè¸ªåŠŸèƒ½"""
    
    def train_with_progress():
        """å¸¦è¿›åº¦è·Ÿè¸ªçš„è®­ç»ƒæ–¹æ³•"""
        for round_idx in range(manager.comm_round):
            
            logging.info(f"################Communication round : {round_idx}")
            
            # ç¬¬ä¸€è½®åˆå§‹åŒ–
            if round_idx == 0:
                named_params = manager.aggregator.get_global_model_params()
                params_type = 'model'
                global_other_params = {}
                shared_params_for_simulation = {}

                # SCAFFOLDæ”¯æŒ
                if (hasattr(manager.args, 'scaffold') and 
                    manager.args.scaffold):
                    c_global_para = (manager.aggregator.c_model_global
                                   .state_dict())
                    global_other_params["c_model_global"] = c_global_para

            # å®¢æˆ·ç«¯é‡‡æ ·
            client_indexes = manager.aggregator.client_sampling(   
                round_idx, manager.args.client_num_in_total,
                manager.args.client_num_per_round)

            update_state_kargs = manager.get_update_state_kargs()

            # è®­ç»ƒå’Œèšåˆ
            named_params, params_type, global_other_params, \
            shared_params_for_simulation = manager.algorithm_train(
                round_idx,
                client_indexes,
                named_params,
                params_type,
                global_other_params,
                update_state_kargs,
                shared_params_for_simulation
            )
            
            # æµ‹è¯•æ¨¡å‹
            test_acc = manager.aggregator.test_on_server_for_round(
                manager.args.VAE_comm_round + round_idx)
            manager.test_acc_list.append(test_acc)
            
            # è®¡ç®—è®­ç»ƒæŒ‡æ ‡
            train_metrics = {
                'loss': 0.0,  # æš‚æ—¶è®¾ä¸º0
                'accuracy': test_acc  # ä½¿ç”¨æµ‹è¯•ç²¾åº¦ä½œä¸ºè¿‘ä¼¼
            }
            
            test_metrics = {
                'loss': 0.0,  # æš‚æ—¶è®¾ä¸º0
                'accuracy': test_acc
            }
            
            # æ›´æ–°è¿›åº¦
            tracker.update_progress(round_idx, train_metrics, test_metrics)
            
            # æ¯20è½®æ‰“å°ä¸€æ¬¡å†å²
            if round_idx % 20 == 0:
                logging.info(f"æµ‹è¯•ç²¾åº¦å†å²: {manager.test_acc_list}")
        
        # ä¿å­˜åˆ†ç±»å™¨
        manager.aggregator.save_classifier()
    
    # æ·»åŠ æ–¹æ³•åˆ°ç®¡ç†å™¨
    manager.train_with_progress = train_with_progress
    return manager


def run_algorithm(cfg, device, algorithm_name, output_dir):
    """è¿è¡ŒæŒ‡å®šçš„è”é‚¦å­¦ä¹ ç®—æ³•"""
    
    # è·å–æ•°æ®é›†åç§°
    dataset_name = getattr(cfg, 'dataset', 'unknown')
    
    # åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨
    tracker = TrainingProgressTracker(
        cfg.comm_round, output_dir, algorithm_name, dataset_name)
    
    # é€‰æ‹©ç®—æ³•ç®¡ç†å™¨
    if algorithm_name == 'FedAvg':
        manager = FedAVGManager(device, cfg)
    elif algorithm_name == 'FedProx':
        manager = FedProxManager(device, cfg)
    elif algorithm_name == 'PFedMe':
        manager = PFedMeManager(device, cfg)
    elif algorithm_name == 'FedNova':
        manager = FedNovaManager(device, cfg)
    elif algorithm_name == 'FedFed':
        manager = FedAVGManager(device, cfg)  # FedFedåŸºäºFedAvgå®ç°
    else:
        raise NotImplementedError(
            f"Algorithm {algorithm_name} not implemented")
    
    # æ·»åŠ è¿›åº¦è·Ÿè¸ªåŠŸèƒ½
    manager = add_progress_tracking_to_manager(manager, tracker)
    
    # è®­ç»ƒ
    try:
        # ä½¿ç”¨å¸¦è¿›åº¦è·Ÿè¸ªçš„è®­ç»ƒæ–¹æ³•
        manager.train_with_progress()
    except Exception as e:
        logging.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        # å¦‚æœå‡ºé”™ï¼Œå°è¯•ä½¿ç”¨åŸå§‹è®­ç»ƒæ–¹æ³•
        logging.warning("å°è¯•ä½¿ç”¨åŸå§‹è®­ç»ƒæ–¹æ³•")
        manager.train()
    
    # ä¿å­˜ç»“æœ
    tracker.save_results()
    
    return tracker


if __name__ == "__main__":
    # è§£æå‚æ•°
    parser = argparse.ArgumentParser()
    args = add_args(parser)
    
    # è®¾ç½®é…ç½®
    cfg = get_cfg()
    cfg.setup(args)
    cfg.mode = 'standalone'
    cfg.server_index = -1
    cfg.client_index = -1
    
    # è®¾ç½®ç®—æ³•ç±»å‹
    if hasattr(args, 'algorithm'):
        cfg.algorithm = args.algorithm
    
    # å¿«é€Ÿæµ‹è¯•æ¨¡å¼
    if args.quick_test:
        cfg.comm_round = 3
        cfg.client_num_in_total = 3
        cfg.client_num_per_round = 2
        cfg.global_epochs_per_round = 1
        print("ğŸš€ å¯ç”¨å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
        logging.info("å¯ç”¨å¿«é€Ÿæµ‹è¯•æ¨¡å¼")
    
    # è®¾ç½®éªŒè¯æ¨¡å¼
    cfg.full_validation = args.full_validation
    
    seed = cfg.seed
    process_id = 0
    
    # æ˜¾ç¤ºé…ç½®
    logging.info(dict(cfg))
    
    # è®¾ç½®è¿›ç¨‹å
    str_process_name = cfg.algorithm + " (standalone):" + str(process_id)
    
    # é…ç½®æ—¥å¿—
    logging_config(args=cfg, process_id=process_id)
    
    hostname = socket.gethostname()
    logging.info("#############process ID = " + str(process_id) +
                 ", host name = " + hostname + "########" +
                 ", process ID = " + str(os.getpid()))

    # è®¾ç½®éšæœºç§å­
    set_random_seed(seed) 
    torch.backends.cudnn.deterministic = True

    # è®¾ç½®è®¾å¤‡
    device = torch.device("cuda:" + str(cfg.gpu_index) 
                         if torch.cuda.is_available() else "cpu")
    
    # é…ç½®wandb
    if cfg.record_tool == 'wandb' and cfg.wandb_record:
        import wandb
        wandb.init(config=args, name=f'{cfg.algorithm}_test',
                   project='FedFed_Yummly28k')
    else: 
        os.environ['WANDB_MODE'] = 'dryrun'

    # è¿è¡Œç®—æ³•
    tracker = run_algorithm(cfg, device, cfg.algorithm, args.output_dir)
    
    print("\nğŸ‰ === è®­ç»ƒå®Œæˆ ===")
    final_test_acc = (tracker.train_history['test_acc'][-1] 
                     if tracker.train_history['test_acc'] else 0)
    print(f"ğŸ¯ æœ€ç»ˆæµ‹è¯•ç²¾åº¦: {final_test_acc:.4f}")
    total_time = (time.time() - tracker.start_time) / 60
    print(f"â±ï¸  æ€»è®­ç»ƒæ—¶é—´: {total_time:.1f}åˆ†é’Ÿ")
    
    logging.info("=== è®­ç»ƒå®Œæˆ ===")
    logging.info(f"æœ€ç»ˆæµ‹è¯•ç²¾åº¦: {final_test_acc:.4f}")
    logging.info(f"æ€»è®­ç»ƒæ—¶é—´: {total_time:.1f}åˆ†é’Ÿ") 